{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b89ee25a",
   "metadata": {},
   "source": [
    "# Dobot Magician Calibration Tutorial\n",
    "\n",
    "Calibrating the Dobot Magician with the camera is essential for accurate position and interaction with the real world environment. This tutorial will guide you through the eye-to-hand calibration process.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "1. Completed the tutorial on [Dobot Magician Robot Arm Control](dobot.ipynb).\n",
    "2. Basic understanding of Python programming.\n",
    "3. Basic knowledge of linear algebra and transformations.\n",
    "4. Intel RealSense camera installed for the full workspace view.\n",
    "5. AprilTag installed on the robot arm (above the gripper).\n",
    "\n",
    "## Learning Objectives\n",
    "1. Understand the concept of eye-to-hand calibration.\n",
    "2. Learn how to perform eye-to-hand calibration using the Dobot Magician and Intel RealSense camera.\n",
    "3. Apply the calibration to improve the accuracy of the robot arm's movements.\n",
    "\n",
    "## Outcome\n",
    "By the end of this tutorial, you will be able to calibrate the Dobot Magician with the Intel RealSense camera, write the transformation matrix from the robot base to the end effector, and verify the calibration by moving the robot arm to the AprilTag position."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ccf637",
   "metadata": {},
   "source": [
    "## Key NumPy Functions for Working with Robot Transformations\n",
    "\n",
    "When working with robot pose data and transformation matrices, several NumPy functions are commonly used:\n",
    "\n",
    "- **`np.arctan2(y, x)`**: Computes the angle (in radians) from the X-axis to the point $(x, y)$. This is useful for determining the orientation of the robot or its end effector in the plane.\n",
    "\n",
    "- **`np.array([...])`**: Creates a NumPy array from a Python list or nested lists. Arrays are used to represent vectors (such as position) and matrices (such as rotation).\n",
    "\n",
    "- **`np.eye(n)`**: Generates an $n \\times n$ identity matrix. This is often used as a starting point for building transformation matrices, since the identity matrix represents no rotation or translation.\n",
    "\n",
    "### Example Usage\n",
    "\n",
    "- To create a rotation matrix for a planar robot, you might use `np.arctan2` to get the angle, then use `np.array` to build the matrix.\n",
    "- To represent the robot's position, use `np.array([x, y, z])`.\n",
    "- To start with a $4 \\times 4$ identity matrix for a transformation, use `np.eye(4)`.\n",
    "\n",
    "Now is your turn to practice these concepts! Write the code to get the transformation matrix from the robot base to the end effector. Run the test to verify your implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fb2de8",
   "metadata": {},
   "source": [
    "## Mathematical Construction of the Transformation Matrix\n",
    "\n",
    "The transformation from one coordinate frame to another is represented by a 4x4 matrix:\n",
    "\n",
    "$$\n",
    "T = \\begin{bmatrix}\n",
    "R & t \\\\\n",
    "0 & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $R$ is the $3 \\times 3$ rotation matrix (describes orientation)\n",
    "- $t$ is the $3 \\times 1$ translation vector (describes position)\n",
    "- The bottom row $[0 \\quad 1]$ ensures the matrix is suitable for homogeneous coordinates.\n",
    "\n",
    "For example, to construct the transformation from the AprilTag to the camera frame:\n",
    "\n",
    "$$\n",
    "T_{\\text{tag} \\to \\text{camera}} = \\begin{bmatrix}\n",
    "R_{\\text{tag} \\to \\text{camera}} & t_{\\text{tag} \\to \\text{camera}} \\\\\n",
    "0 & 1\n",
    "\\end{bmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b8fe0e",
   "metadata": {},
   "source": [
    "## Get the Robot Base to End Effector Transformation Matrix\n",
    "\n",
    "The transformation matrix from the robot base to the end effector can be constructed directly using the pose information provided by the robot's sensors.\n",
    "\n",
    "### Steps to Construct the Transformation Matrix\n",
    "\n",
    "1. **Pose Acquisition**: The robot controller or API typically provides the current pose of the end effector, including its position $(x, y, z)$ and orientation (often as a rotation matrix or quaternion).\n",
    "\n",
    "2. **Matrix Construction**: Combine the position and orientation into a $4 \\times 4$ homogeneous transformation matrix:\n",
    "   $$\n",
    "   T_{\\text{base} \\to \\text{ee}} = \\begin{bmatrix}\n",
    "   R & t \\\\\n",
    "   0 & 1\n",
    "   \\end{bmatrix}\n",
    "   $$\n",
    "   - $R$ is the $3 \\times 3$ rotation matrix representing the end effector's orientation.\n",
    "   - $t$ is the $3 \\times 1$ translation vector $[x, y, z]^T$ representing the end effector's position.\n",
    "\n",
    "## Practice 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aadb3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pydobotplus import Dobot\n",
    "\n",
    "def get_robot_base_to_ee(pose):\n",
    "    \"\"\"\n",
    "    This function returns the transformation matrix from the robot base to the end effector.\n",
    "    The transformation matrix is a 4x4 matrix that includes both rotation and translation.\n",
    "\n",
    "    Args:\n",
    "        pose (dict): A dictionary containing the robot's pose data, including position and joint information.\n",
    "                    This includes 'position' (a 3-element list) - x, y, z coordinates\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: A 4x4 transformation matrix representing the pose of the end effector\n",
    "                    in the robot base frame.\n",
    "    \"\"\"\n",
    "    # Your implementation here\n",
    "    return np.eye(4) # Edit this line to return the actual transformation matrix\n",
    "\n",
    "print(\"Transformation Matrix from Robot Base to End Effector:\")\n",
    "port = \"/dev/ttyACM0\" # Edit this line to match your Dobot's port\n",
    "dobot = Dobot(port=port)\n",
    "T_base_to_ee = get_robot_base_to_ee(dobot.get_pose())\n",
    "print(T_base_to_ee)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28dec3d2",
   "metadata": {},
   "source": [
    "## Find the Transformation Matrix from AprilTag to End-effector\n",
    "\n",
    "The transformation matrix from the gripper (end effector) to the AprilTag describes the position and orientation of the AprilTag relative to the gripper's coordinate frame. When the x-axes of both frames are aligned, you can measure the translation and orientation offsets using a ruler or caliper.\n",
    "\n",
    "- The first row means the tag's x-axis is anti-aligned with the gripper's x-axis, and the tag is 30 mm away along x.\n",
    "- The second row means the y-axes are aligned, with no offset.\n",
    "- The third row means the tag's z-axis is opposite to the gripper's z-axis, and the tag is 153 mm away along z.\n",
    "- The fourth row ensures homogeneous coordinates.\n",
    "\n",
    "This matrix (`gHt`) allows you to transform points between the gripper and AprilTag frames. The tag size (0.792) is used for calibration and pose estimation.\n",
    "\n",
    "## Practice 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1dd5f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edit this matrix based on your measurement\n",
    "gHt = np.array([\n",
    "    [1, 0, 0, 0],\n",
    "    [0, 1, 0, 0],\n",
    "    [0, 0, 1, 0],\n",
    "    [0, 0, 0, 1]\n",
    "], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19b1ed4",
   "metadata": {},
   "source": [
    "# AprilTag & Camera\n",
    "## Step 1: Turn on the Intel RealSense Camera\n",
    "Intel RealSense camera is a depth camera that provides RGB frame and depth frame.   \n",
    "\n",
    "To turn on the camera, you should do the following:\n",
    "1. Connect the Intel RealSense camera to your computer.\n",
    "2. Run the code to check the camera serial (Important step if you have multiple cameras connected)\n",
    "3. Run the code to start the camera stream and display the RGB frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6356f89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyrealsense2 as rs\n",
    "\n",
    "# Check the camera serial\n",
    "ctx = rs.context()\n",
    "serials = [device.get_info(rs.camera_info.serial_number) for device in ctx.query_devices()]\n",
    "for i, serial in enumerate(serials):\n",
    "    print(f\"Camera Serial {i}: {serial}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c215d61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "import pyrealsense2 as rs\n",
    "\n",
    "camera_serial = \"218622277520\" # Replace with your camera serial\n",
    "\n",
    "# Initialize the camera pipeline\n",
    "ctx = rs.context()\n",
    "pipeline = rs.pipeline(ctx)\n",
    "config = rs.config()\n",
    "config.enable_device(camera_serial)\n",
    "config.enable_stream(rs.stream.color, 640, 480, rs.format.bgr8, 30)\n",
    "config.enable_stream(rs.stream.depth, 640, 480, rs.format.z16, 30)\n",
    "profile = pipeline.start(config)\n",
    "align = rs.align(rs.stream.color)\n",
    "\n",
    "# Display a live camera stream\n",
    "try:\n",
    "    for _ in range(100):  # Show 100 frames, adjust as needed\n",
    "        frames = pipeline.wait_for_frames()\n",
    "        aligned_frames = align.process(frames)\n",
    "        color_frame = aligned_frames.get_color_frame()\n",
    "        if color_frame:\n",
    "            color_image = np.asanyarray(color_frame.get_data())\n",
    "            plt.imshow(cv2.cvtColor(color_image, cv2.COLOR_BGR2RGB))\n",
    "            plt.title(\"RGB Frame\")\n",
    "            plt.axis('off')\n",
    "            clear_output(wait=True)\n",
    "            display(plt.gcf())\n",
    "            plt.clf()\n",
    "finally:\n",
    "    pipeline.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b268f6",
   "metadata": {},
   "source": [
    "Now, you should see the live camera stream.  \n",
    "\n",
    "## Step 2: Find the AprilTag\n",
    "AprilTag is a visual fiducial system that allows the robot arm to recognize its position in the workspace. The AprilTag is attached to the robot arm above the gripper.\n",
    "You can use the `pupil-apriltags` library to detect the AprilTag in the camera frame. The library provides functions to detect AprilTags and get their positions in the camera coordinate system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d913889c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pupil_apriltags import Detector\n",
    "import cv2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize AprilTag detector\n",
    "detector = Detector()\n",
    "\n",
    "# Convert color image to grayscale for AprilTag detection\n",
    "gray_image = cv2.cvtColor(color_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Detect AprilTags in the grayscale image\n",
    "tags = detector.detect(gray_image, estimate_tag_pose=False, camera_params=None, tag_size=None)\n",
    "\n",
    "# Draw detected tags on the image\n",
    "image_with_tags = color_image.copy()\n",
    "for tag in tags:\n",
    "    corners = tag.corners.astype(int)\n",
    "    for i in range(4):\n",
    "        pt1 = tuple(corners[i])\n",
    "        pt2 = tuple(corners[(i + 1) % 4])\n",
    "        cv2.line(image_with_tags, pt1, pt2, (0, 255, 0), 2)\n",
    "    # Draw center\n",
    "    center = tuple(tag.center.astype(int))\n",
    "    cv2.circle(image_with_tags, center, 5, (0, 0, 255), -1)\n",
    "    # Put tag ID\n",
    "    cv2.putText(image_with_tags, str(tag.tag_id), center, cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n",
    "\n",
    "# Display the image\n",
    "plt.imshow(cv2.cvtColor(image_with_tags, cv2.COLOR_BGR2RGB))\n",
    "plt.title(\"AprilTag Detection\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374e9667",
   "metadata": {},
   "source": [
    "## Finding the Transformation from Camera to AprilTag\n",
    "The first step of building this connection is to find the transformation matrix from the camera to the AprilTag. This is done by detecting the AprilTag in the camera frame and calculating its position and orientation. Thanks to the `pupil-apriltags` library, you can easily detect the AprilTag and get its translation vector and rotation matrix.\n",
    "\n",
    "To construct the transformation matrix, you will need to combine the translation vector and rotation matrix into a single transformation matrix. The translation vector is the position of the AprilTag in the camera coordinate system, and the rotation matrix is the orientation of the AprilTag in the camera coordinate system.  \n",
    "\n",
    "## Practices 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7585f290",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pupil_apriltags import Detector\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Get camera intrinsics from RealSense2\n",
    "intr = profile.get_stream(rs.stream.color).as_video_stream_profile().get_intrinsics()\n",
    "camera_params = [intr.fx, intr.fy, intr.ppx, intr.ppy]\n",
    "\n",
    "tag_size = 0.0792  # Tag size in meters (!! Measure this by yourself)\n",
    "\n",
    "# Initialize AprilTag detector\n",
    "detector = Detector()\n",
    "\n",
    "# Convert color image to grayscale for AprilTag detection\n",
    "gray_image = cv2.cvtColor(color_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Detect AprilTags and estimate pose\n",
    "tags = detector.detect(\n",
    "    gray_image,\n",
    "    estimate_tag_pose=True,\n",
    "    camera_params=camera_params,\n",
    "    tag_size=tag_size\n",
    ")\n",
    "\n",
    "for tag in tags:\n",
    "    print(f\"Tag ID: {tag.tag_id}\")\n",
    "    print(\"Translation (x, y, z) in meters:\", tag.pose_t)\n",
    "    print(\"Rotation matrix (3x3):\\n\", tag.pose_R)\n",
    "    # You can construct the 4x4 transformation matrix \n",
    "    # HERE!!!!!\n",
    "    print(\"Transformation matrix (tag to camera):\\n\", T_tag_to_camera)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c79fcdd",
   "metadata": {},
   "source": [
    "# Step 3: Calculate the transformation matrix\n",
    "\n",
    "Let's break down how to connect the robot base and the camera using simple steps.\n",
    "\n",
    "Imagine you want to know where the camera is compared to the robot base. You can't measure this directly, but you can use a chain of connections:\n",
    "\n",
    "1. **Robot base to end effector**: This is the position of the robot's moving part (the end effector, like the gripper) relative to the robot base. The robot can tell you this using its own sensors.\n",
    "2. **End effector to AprilTag**: The AprilTag is attached to the end effector. You measure how far and in what direction the tag is from the end effector. This is usually a fixed value, since you attach the tag yourself.\n",
    "3. **AprilTag to camera**: The camera sees the AprilTag and can calculate where the tag is in its own view.\n",
    "\n",
    "To get the full connection from the robot base to the camera, you combine these three steps:\n",
    "\n",
    "$$\\text{Robot base to camera} = \\text{Robot base to end effector} \\times \\text{End effector to AprilTag} \\times \\text{AprilTag to camera}$$\n",
    "\n",
    "Or, using symbols:\n",
    "\n",
    "$$T_{\\text{base} \\to \\text{camera}} = T_{\\text{base} \\to \\text{ee}} \\times T_{\\text{ee} \\to \\text{tag}} \\times T_{\\text{tag} \\to \\text{camera}}$$\n",
    "\n",
    "![Calibration Diagram](Screenshot%20from%202025-07-14%2010-47-33.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5bc1e9",
   "metadata": {},
   "source": [
    "# Final Steps: Construct the robot base to camera transformation matrix\n",
    "\n",
    "To construct the final transformation matrix from the robot base to the camera, you will need to combine the previously calculated matrices:\n",
    "\n",
    "$$\n",
    "T_{base}^{camera} =  T_{AprilTag}^{camera} \\cdot T_{gripper}^{AprilTag}  \\cdot T_{base}^{gripper}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $T_{base}^{gripper}$ is the transformation matrix from the robot base to the end effector.\n",
    "- $T_{gripper}^{AprilTag}$ is the transformation matrix from the end effector to the AprilTag.\n",
    "- $T_{AprilTag}^{camera}$ is the transformation matrix from the AprilTag to the camera.\n",
    "- $T_{base}^{camera}$ is the final transformation matrix from the robot base to the camera.\n",
    "\n",
    "\n",
    "### Essential NumPy Functions for Matrix Calculations\n",
    "\n",
    "To combine transformation matrices and perform robot calibration, you will use the following NumPy functions:\n",
    "\n",
    "- **`np.dot(A, B)`** or **`A @ B`**: Matrix multiplication. Use this to multiply transformation matrices in the correct order.\n",
    "- **`np.linalg.inv(A)`**: Computes the inverse of a matrix. Useful if you need to reverse a transformation (e.g., from camera to AprilTag).\n",
    "- **`np.eye(n)`**: Creates an identity matrix of size $n \\times n$. Often used to initialize transformation matrices.\n",
    "- **`np.array([...])`**: Converts a Python list to a NumPy array, which is required for matrix operations.\n",
    "\n",
    "#### Example: Matrix Multiplication\n",
    "\n",
    "To combine the matrices for the final transformation:\n",
    "\n",
    "```python\n",
    "T_base_to_camera = T_base_to_ee @ gHt @ T_tag_to_camera\n",
    "```\n",
    "\n",
    "This multiplies the matrices in the correct order to get the transformation from the robot base to the camera.\n",
    "\n",
    "#### Example: Inverse Transformation\n",
    "\n",
    "If you need the transformation from camera to robot base:\n",
    "\n",
    "```python\n",
    "T_camera_to_base = np.linalg.inv(T_base_to_camera)\n",
    "```\n",
    "\n",
    "Now, try to write the code to get the transformation matrix from the robot base to the camera. Use the provided functions and matrices to perform the calculations. Once you have your implementation, run it on the robotics arm to verify the formula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6d39d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_robot_base_to_camera(T_base_to_ee, T_ee_to_tag, T_tag_to_camera):\n",
    "    # Your implementation here\n",
    "    T_base_to_camera = np.eye(4) # Edit this line to return the actual transformation matrix\n",
    "    return T_base_to_camera"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2407db2f",
   "metadata": {},
   "source": [
    "# Verifying the Transformation Matrix\n",
    "\n",
    "To verify the transformation matrix, we can use the apriltag detection and the camera stream to check if the robot arm can accurately reach the AprilTag position.  \n",
    "\n",
    "Now, paste your code to `calibration/utils.py` and run the code to verify the transformation matrix. \n",
    "\n",
    "First, use command `python3 scripts/check_port.py` to check the dobot port and the camera serial. This will ensure that the robot arm and camera are connected properly.\n",
    "\n",
    "Use command `python3 calibration/calibration_simplified.py` to run the data collection script. This script will collect the transformation matrices from the robot base to the camera for multiple AprilTag detections and calculate the average transformation matrix.\n",
    "\n",
    "Use command `python3 calibration/calibration_validate.py` to run the validation script. This script will use the average transformation matrix to validate the calibration by moving the robot arm to the AprilTag position and checking if it can reach it accurately.\n",
    "\n",
    "If your function writes the transformation matrix correctly, the robot arm should be able to reach the AprilTag position accurately. If not, you may need to adjust the transformation matrix formula."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
